{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHU1BCz+wxtLf9iarMBm+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAkbarBadri/topic-modelling/blob/master/topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup"
      ],
      "metadata": {
        "id": "OBcnAguKWKeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9Mavb3xsUFG3"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ],
      "metadata": {
        "id": "LXmYWtJqUPda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aJMy1d9UQEo",
        "outputId": "dacf755a-3e27-4fb7-bdb9-b0ded36c3555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train['data'][0].replace('\\n','')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "-bc96hzQUdvH",
        "outputId": "1bb30ffb-fe50-4114-c496-06a49c1bcb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)Subject: WHAT car is this!?Nntp-Posting-Host: rac3.wam.umd.eduOrganization: University of Maryland, College ParkLines: 15 I was wondering if anyone out there could enlighten me on this car I sawthe other day. It was a 2-door sports car, looked to be from the late 60s/early 70s. It was called a Bricklin. The doors were really small. In addition,the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, yearsof production, where this car is made, history, or whatever info youhave on this funky looking car, please e-mail.Thanks,- IL   ---- brought to you by your neighborhood Lerxst ----\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOPIC MODELLING"
      ],
      "metadata": {
        "id": "1iJj6RtSaxCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "mKP0_D06bBG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data visualisation and manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "#configure\n",
        "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
        "%matplotlib inline  \n",
        "style.use('fivethirtyeight')\n",
        "sns.set(style='whitegrid',color_codes=True)\n",
        "\n",
        "#import nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "#preprocessing\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
        "\n",
        "# for named entity recognition (NER)\n",
        "from nltk import ne_chunk\n",
        "\n",
        "# vectorizers for creating the document-term-matrix (DTM)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
      ],
      "metadata": {
        "id": "Z5mEmukvWdap"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzaAE8cPYqjw",
        "outputId": "59c2981e-64b7-44eb-a6e3-da26dba937c7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stop-words\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxe_sIDKWkDJ",
        "outputId": "f282f5d3-9717-4d57-cc42-d14152b4bd49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(headline):\n",
        "  le=WordNetLemmatizer()\n",
        "  headline = headline.replace('\\n', '')\n",
        "  word_tokens = word_tokenize(headline)\n",
        "  tokens = [le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
        "  cleaned_text = \" \".join(tokens)\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "JB9EyNN9WJo9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(newsgroups_train['data'], columns = ['data'])"
      ],
      "metadata": {
        "id": "4wmmQFZLYAWO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['data'] = df['data'].apply(clean_text)"
      ],
      "metadata": {
        "id": "c0OqlL0gXCgZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['data'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nrQhYaUdYkSg",
        "outputId": "86dc85aa-cf5c-4425-9d05-17ceeb4a731d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'From lerxst wam.umd.edu thing Subject WHAT Nntp-Posting-Host rac3.wam.umd.eduOrganization University Maryland College ParkLines wondering anyone could enlighten sawthe 2-door sport looked late 60s/early called Bricklin door really small addition front bumper separate rest body This know anyone tellme model name engine spec yearsof production made history whatever info youhave funky looking please e-mail.Thanks brought neighborhood Lerxst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)"
      ],
      "metadata": {
        "id": "8i5pG4vjYPjL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect_text=vect.fit_transform(df['data'])"
      ],
      "metadata": {
        "id": "S0tv31IfZ10I"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Semantic Analysis (LSA)"
      ],
      "metadata": {
        "id": "UxLAbSA9WOGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
        "\n",
        "lsa_top=lsa_model.fit_transform(vect_text)"
      ],
      "metadata": {
        "id": "rD4gd-a9Z4S3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# most important words for each topic\n",
        "vocab = vect.get_feature_names_out()\n",
        "\n",
        "for i, comp in enumerate(lsa_model.components_):\n",
        "    vocab_comp = zip(vocab, comp)\n",
        "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_words:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6_slxHMaNpQ",
        "outputId": "3b98e13a-c2a4-4e2e-f6d4-c4e9a2da98e6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: \n",
            "edu com would writes article subject posting like university host \n",
            "\n",
            "Topic 1: \n",
            "edu file windows university thanks card host posting window drive \n",
            "\n",
            "Topic 2: \n",
            "edu cs university game team cc pitt player uiuc year \n",
            "\n",
            "Topic 3: \n",
            "com article writes edu netcom hp posting host pitt inc \n",
            "\n",
            "Topic 4: \n",
            "game team ca player year hockey play season toronto league \n",
            "\n",
            "Topic 5: \n",
            "nasa gov space research access digex center moon orbit station \n",
            "\n",
            "Topic 6: \n",
            "ac uk cs file co window ca pitt gordon banks \n",
            "\n",
            "Topic 7: \n",
            "pitt cs gordon banks computer chip pittsburgh univ clipper soon \n",
            "\n",
            "Topic 8: \n",
            "chip clipper encryption key government game escrow state netcom phone \n",
            "\n",
            "Topic 9: \n",
            "ac uk drive scsi chip clipper co hard encryption key \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Dirichlet Allocation (LDA)"
      ],
      "metadata": {
        "id": "benCoHy0a0LW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_hO-RWlqaRgU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}