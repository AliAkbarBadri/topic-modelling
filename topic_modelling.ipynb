{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlFvpP/wM9di58wI8eX3rG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAkbarBadri/topic-modelling/blob/master/topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup"
      ],
      "metadata": {
        "id": "OBcnAguKWKeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9Mavb3xsUFG3"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ],
      "metadata": {
        "id": "LXmYWtJqUPda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aJMy1d9UQEo",
        "outputId": "dacf755a-3e27-4fb7-bdb9-b0ded36c3555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train['data'][0].replace('\\n','')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "-bc96hzQUdvH",
        "outputId": "1bb30ffb-fe50-4114-c496-06a49c1bcb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)Subject: WHAT car is this!?Nntp-Posting-Host: rac3.wam.umd.eduOrganization: University of Maryland, College ParkLines: 15 I was wondering if anyone out there could enlighten me on this car I sawthe other day. It was a 2-door sports car, looked to be from the late 60s/early 70s. It was called a Bricklin. The doors were really small. In addition,the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, yearsof production, where this car is made, history, or whatever info youhave on this funky looking car, please e-mail.Thanks,- IL   ---- brought to you by your neighborhood Lerxst ----\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOPIC MODELLING"
      ],
      "metadata": {
        "id": "1iJj6RtSaxCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "mKP0_D06bBG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data visualisation and manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "#configure\n",
        "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
        "%matplotlib inline  \n",
        "style.use('fivethirtyeight')\n",
        "sns.set(style='whitegrid',color_codes=True)\n",
        "\n",
        "#import nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "#preprocessing\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
        "\n",
        "# for named entity recognition (NER)\n",
        "from nltk import ne_chunk\n",
        "\n",
        "# vectorizers for creating the document-term-matrix (DTM)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
      ],
      "metadata": {
        "id": "Z5mEmukvWdap"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzaAE8cPYqjw",
        "outputId": "59c2981e-64b7-44eb-a6e3-da26dba937c7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stop-words\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxe_sIDKWkDJ",
        "outputId": "f282f5d3-9717-4d57-cc42-d14152b4bd49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(headline):\n",
        "  le=WordNetLemmatizer()\n",
        "  headline = headline.replace('\\n', '')\n",
        "  word_tokens = word_tokenize(headline)\n",
        "  tokens = [le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
        "  cleaned_text = \" \".join(tokens)\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "JB9EyNN9WJo9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(newsgroups_train['data'], columns = ['data'])"
      ],
      "metadata": {
        "id": "4wmmQFZLYAWO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['data'] = df['data'].apply(clean_text)"
      ],
      "metadata": {
        "id": "c0OqlL0gXCgZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['data'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nrQhYaUdYkSg",
        "outputId": "86dc85aa-cf5c-4425-9d05-17ceeb4a731d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'From lerxst wam.umd.edu thing Subject WHAT Nntp-Posting-Host rac3.wam.umd.eduOrganization University Maryland College ParkLines wondering anyone could enlighten sawthe 2-door sport looked late 60s/early called Bricklin door really small addition front bumper separate rest body This know anyone tellme model name engine spec yearsof production made history whatever info youhave funky looking please e-mail.Thanks brought neighborhood Lerxst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)"
      ],
      "metadata": {
        "id": "8i5pG4vjYPjL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect_text=vect.fit_transform(df['data'])"
      ],
      "metadata": {
        "id": "S0tv31IfZ10I"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Semantic Analysis (LSA)"
      ],
      "metadata": {
        "id": "UxLAbSA9WOGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)"
      ],
      "metadata": {
        "id": "kIiM-okegkhV"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
        "vect_text=vect.fit_transform(df['data'])\n",
        "\n",
        "lsa_top=lsa_model.fit_transform(vect_text)\n",
        "\n",
        "# most important words for each topic\n",
        "vocab = vect.get_feature_names_out()\n",
        "\n",
        "for i, comp in enumerate(lsa_model.components_):\n",
        "    vocab_comp = zip(vocab, comp)\n",
        "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_words:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6_slxHMaNpQ",
        "outputId": "0c638b28-7b88-4c09-c48a-1d0d728ef0bc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: \n",
            "edu com would writes article subject posting like university host \n",
            "\n",
            "Topic 1: \n",
            "edu file windows university thanks card host posting window drive \n",
            "\n",
            "Topic 2: \n",
            "edu cs university game team cc pitt player uiuc year \n",
            "\n",
            "Topic 3: \n",
            "com article writes edu netcom hp posting host pitt inc \n",
            "\n",
            "Topic 4: \n",
            "game team ca player year hockey play season toronto league \n",
            "\n",
            "Topic 5: \n",
            "nasa gov space research access digex center moon orbit station \n",
            "\n",
            "Topic 6: \n",
            "ac uk cs file co window ca pitt gordon banks \n",
            "\n",
            "Topic 7: \n",
            "pitt cs gordon banks computer chip pittsburgh univ clipper soon \n",
            "\n",
            "Topic 8: \n",
            "chip clipper encryption key government game escrow state netcom phone \n",
            "\n",
            "Topic 9: \n",
            "ac uk drive scsi chip clipper co hard encryption key \n",
            "\n",
            "2.1852498054504395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
        "vect_text=vect.fit_transform(newsgroups_train['data'])\n",
        "\n",
        "lsa_top=lsa_model.fit_transform(vect_text)\n",
        "\n",
        "# most important words for each topic\n",
        "vocab = vect.get_feature_names_out()\n",
        "\n",
        "for i, comp in enumerate(lsa_model.components_):\n",
        "    vocab_comp = zip(vocab, comp)\n",
        "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_words:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpdek1t4jb_l",
        "outputId": "33292611-f2be-4d14-b47d-03924709935d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: \n",
            "edu com would writes article one subject lines organization university \n",
            "\n",
            "Topic 1: \n",
            "god people com would one jesus think us government say \n",
            "\n",
            "Topic 2: \n",
            "com windows netcom hp access inc uk ibm dos window \n",
            "\n",
            "Topic 3: \n",
            "com edu article writes netcom posting nntp hp host pitt \n",
            "\n",
            "Topic 4: \n",
            "ca team game year nasa hockey gov toronto players games \n",
            "\n",
            "Topic 5: \n",
            "key nasa clipper chip encryption gov government edu space keys \n",
            "\n",
            "Topic 6: \n",
            "uk nasa ac gov space co cs __ science research \n",
            "\n",
            "Topic 7: \n",
            "nasa gov god space jesus windows bible research ca center \n",
            "\n",
            "Topic 8: \n",
            "cs pitt gordon science computer windows pittsburgh ca univ soon \n",
            "\n",
            "Topic 9: \n",
            "ca key god clipper chip encryption escrow keys canada jesus \n",
            "\n",
            "2.531376361846924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Dirichlet Allocation (LDA)"
      ],
      "metadata": {
        "id": "benCoHy0a0LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=42,max_iter=1) "
      ],
      "metadata": {
        "id": "_hO-RWlqaRgU"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
        "vect_text=vect.fit_transform(newsgroups_train['data'])\n",
        "\n",
        "lda_top=lda_model.fit_transform(vect_text)\n",
        "\n",
        "# most important words for each topic\n",
        "vocab = vect.get_feature_names()\n",
        "\n",
        "for i, comp in enumerate(lda_model.components_):\n",
        "    vocab_comp = zip(vocab, comp)\n",
        "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_words:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSKHerU9biuI",
        "outputId": "12831f08-5698-4bb3-d1b5-ed7364b1d4ab"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: \n",
            "colorado frank 000 500 vs org rate edu year hell \n",
            "\n",
            "Topic 1: \n",
            "uiuc israel cso israeli virginia columbia jewish harvard jews policy \n",
            "\n",
            "Topic 2: \n",
            "turkish armenians armenian turkey armenia greek russian uucp million population \n",
            "\n",
            "Topic 3: \n",
            "edu com windows lines subject organization university thanks posting host \n",
            "\n",
            "Topic 4: \n",
            "andrew cmu pittsburgh ax 145 cx engineering 0d ah edu \n",
            "\n",
            "Topic 5: \n",
            "god jesus bible church christian christians christ rutgers faith christianity \n",
            "\n",
            "Topic 6: \n",
            "edu team game ca nasa pitt hockey year cs games \n",
            "\n",
            "Topic 7: \n",
            "com edu would writes one article people like subject organization \n",
            "\n",
            "Topic 8: \n",
            "de hp com edu thanks uni mail lines subject organization \n",
            "\n",
            "Topic 9: \n",
            "edu caltech keith cwru cleveland freenet com sgi writes morality \n",
            "\n",
            "7.015937805175781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
        "vect_text=vect.fit_transform(df['data'])\n",
        "\n",
        "lda_top=lda_model.fit_transform(vect_text)\n",
        "\n",
        "# most important words for each topic\n",
        "vocab = vect.get_feature_names()\n",
        "\n",
        "for i, comp in enumerate(lda_model.components_):\n",
        "    vocab_comp = zip(vocab, comp)\n",
        "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_words:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8rgoPdVjqlX",
        "outputId": "f0de2b93-13cc-45fc-cb56-514e2dd38bd9"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: \n",
            "utexas font scsi austin max texas cc cs cx 34u \n",
            "\n",
            "Topic 1: \n",
            "uiuc columbia cso illinois insurance sun edu cc rochester harvard \n",
            "\n",
            "Topic 2: \n",
            "space chip com clipper nasa netcom encryption key edu gov \n",
            "\n",
            "Topic 3: \n",
            "edu people com would writes article think jesus subject know \n",
            "\n",
            "Topic 4: \n",
            "team game player pitt edu hockey cmu gordon season andrew \n",
            "\n",
            "Topic 5: \n",
            "uk ac cwru cleveland freenet co western edu host case \n",
            "\n",
            "Topic 6: \n",
            "edu ohio nasa caltech buffalo keith gatech state sgi gov \n",
            "\n",
            "Topic 7: \n",
            "file window mit windows program server motif image color problem \n",
            "\n",
            "Topic 8: \n",
            "edu com card subject thanks drive university posting mail host \n",
            "\n",
            "Topic 9: \n",
            "com edu writes would article like subject think bike time \n",
            "\n",
            "6.89482307434082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BertTopic"
      ],
      "metadata": {
        "id": "SXLKhlJng4w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q bertopic"
      ],
      "metadata": {
        "id": "OAJFO7Jkbpms"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n"
      ],
      "metadata": {
        "id": "ovGQldYcg_tH"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "topics, probs = topic_model.fit_transform(newsgroups_train['data'])\n",
        "\n",
        "print(time.time() - start_time)"
      ],
      "metadata": {
        "id": "YFK34B7RhZrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[topic_model.get_topics()[i] for i in range(10)]"
      ],
      "metadata": {
        "id": "eRN0NLDG6kto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model2 = BERTopic()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "topics2, probs2 = topic_model2.fit_transform(df['data'])\n",
        "\n",
        "print(time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMwEiwEdiZSh",
        "outputId": "c2bd4a8f-1386-430c-b359-3dfad435df16"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2534.42577791214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[topic_model2.get_topics()[i] for i in range(10)]"
      ],
      "metadata": {
        "id": "nKka9bRE5xaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtxY6VDvmAYi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}